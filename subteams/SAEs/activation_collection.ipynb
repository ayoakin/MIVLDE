{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64870161-5450-40a4-bcd8-d99c3b625400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import enum\n",
    "import pickle\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from mishax import ast_patcher\n",
    "from mishax import safe_greenlet\n",
    "\n",
    "import odeformer\n",
    "from odeformer.model import SymbolicTransformerRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45561b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Site(enum.StrEnum):\n",
    "    \"\"\"Instrumentation sites within an ODEFormer forward pass.\"\"\"\n",
    "    # Attention sites\n",
    "    QUERY, KEY, VALUE, ATTN_SCORES, ATTN_PROBS, ATTN_OUTPUT, ATTN_MLP_OUTPUT, POST_ATTN_RESIDUAL = (\n",
    "        enum.auto(), enum.auto(), enum.auto(), enum.auto(), enum.auto(), enum.auto(), enum.auto(), enum.auto()\n",
    "    )\n",
    "\n",
    "    # Layer norm sites\n",
    "    PRE_ATTN_LAYERNORM, PRE_MLP_LAYERNORM = enum.auto(), enum.auto()\n",
    "\n",
    "    # MLP sites\n",
    "    MLP_INPUT, MLP_HIDDEN, MLP_OUTPUT, POST_MLP_RESIDUAL = enum.auto(), enum.auto(), enum.auto(), enum.auto()\n",
    "\n",
    "    # Cross attention (decoder only)\n",
    "    CROSS_ATTN_SCORES, CROSS_ATTN_PROBS, CROSS_ATTN_OUTPUT = enum.auto(), enum.auto(), enum.auto()\n",
    "\n",
    "class ModulePathMapper:\n",
    "    \"\"\"Maps modules to their hierarchical paths within the model.\"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.path_map = {}\n",
    "        self.model = model\n",
    "        self._build_path_map()\n",
    "\n",
    "    def _build_path_map(self):\n",
    "        \"\"\"Constructs the module-to-path mapping.\"\"\"\n",
    "        model = getattr(self.model, 'model', self.model)\n",
    "        \n",
    "        for section in ['encoder', 'decoder']:\n",
    "            module = getattr(model, section, None)\n",
    "            if module:\n",
    "                for name, sub_module in module.named_modules():\n",
    "                    self.path_map[id(sub_module)] = f\"{section}.{name if name else 'outer'}\"\n",
    "\n",
    "    def get_layer_path(self, module: nn.Module, accessing_component: str = None) -> str:\n",
    "        \"\"\"Returns the full hierarchical path including accessed component if provided.\"\"\"\n",
    "        base_path = self.path_map.get(id(module))\n",
    "        return f\"{base_path}.{accessing_component}\" if base_path and accessing_component else base_path\n",
    "\n",
    "_path_mapper = None\n",
    "\n",
    "def _tag(module: nn.Module, site: Site, value: torch.Tensor, accessing: str = None) -> torch.Tensor:\n",
    "    \"\"\"Tags a value at a particular site for instrumentation.\"\"\"\n",
    "    try:\n",
    "        parent = safe_greenlet.getparent()\n",
    "        if parent is None:\n",
    "            return value\n",
    "\n",
    "        # Get full path including component\n",
    "        path = None\n",
    "        if _path_mapper is not None:\n",
    "            path = _path_mapper.get_layer_path(module, accessing)\n",
    "\n",
    "        ret = parent.switch((site, value, path))\n",
    "        return ret if ret is not None else value\n",
    "    except Exception as e:\n",
    "        print(f\"Error in tag at {site}: {e}\")\n",
    "        return value\n",
    "\n",
    "def collect_activations_during_fit(model, times, trajectories):\n",
    "    \"\"\"Collects activations during model training.\"\"\"\n",
    "    global _path_mapper\n",
    "    _path_mapper = ModulePathMapper(model)\n",
    "    return collect_activations(lambda: model.fit(times, trajectories))\n",
    "\n",
    "def install():\n",
    "    \"\"\"Installs patches for instrumentation.\"\"\"\n",
    "    print(\"Installing patches...\", end=' ', flush=True)\n",
    "    \n",
    "    PREFIX = f\"from {__name__} import Site, _tag as tag\"\n",
    "    \n",
    "    patcher = ast_patcher.ModuleASTPatcher(\n",
    "        odeformer.model.transformer,\n",
    "        ast_patcher.PatchSettings(prefix=PREFIX),\n",
    "        MultiHeadAttention=[\n",
    "            \"scores = torch.matmul(q, k.transpose(2, 3))\",\n",
    "            \"scores = tag(self, Site.ATTN_SCORES, torch.matmul(q, k.transpose(2, 3)), accessing='scores')\",\n",
    "            \n",
    "            \"weights = F.softmax(scores.float(), dim=-1).type_as(scores)\",\n",
    "            \"weights = tag(self, Site.ATTN_PROBS, F.softmax(scores.float(), dim=-1).type_as(scores), accessing='weights')\",\n",
    "\n",
    "            \"context = torch.matmul(weights, v)\",\n",
    "            \"context = tag(self, Site.ATTN_OUTPUT, torch.matmul(weights, v), accessing='context')\",\n",
    "        ],\n",
    "        TransformerModel=[\n",
    "            \"\"\"\n",
    "            attn = self.encoder_attn[i](\n",
    "                    tensor, src_mask, kv=src_enc, use_cache=use_cache\n",
    "                )\n",
    "            \"\"\",\n",
    "            \"\"\"\n",
    "            attn = tag(\n",
    "                        self, Site.ATTN_MLP_OUTPUT, \n",
    "                        self.encoder_attn[i](\n",
    "                            tensor, src_mask, kv=src_enc, use_cache=use_cache\n",
    "                        ),\n",
    "                        accessing=f'cross_attention{i}'\n",
    "                    )\n",
    "            \"\"\",\n",
    "            \n",
    "            \"attn = self.attentions[i](tensor, attn_mask, use_cache=use_cache)\",\n",
    "            \"attn = tag(self, Site.ATTN_MLP_OUTPUT, self.attentions[i](tensor, attn_mask, use_cache=use_cache), accessing=f'attention_layer{i}')\"\n",
    "        ],\n",
    "        TransformerFFN=[\n",
    "            \"x = self.lin1(input)\",\n",
    "            \"x = self.lin1(tag(self, Site.MLP_INPUT, input, accessing='input'))\",\n",
    "            \n",
    "            \"x = self.lin2(x)\",\n",
    "            \"x = tag(self, Site.MLP_OUTPUT, self.lin2(x), accessing='output')\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        patcher.install()\n",
    "        print(\"Patches installed successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error installing patches: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    return patcher\n",
    "\n",
    "def _process_activations(activations):\n",
    "    \"\"\"Processes collected activations into a structured format.\"\"\"\n",
    "    processed = {}\n",
    "    for site, name_data in activations.items():\n",
    "        processed[site] = {}\n",
    "        for name, tensors in name_data.items():\n",
    "            grouped = defaultdict(list)\n",
    "            for tensor in tensors:\n",
    "                grouped[tuple(tensor.shape)].append(tensor)\n",
    "            processed[site][name] = {\n",
    "                shape: torch.stack(tensors) \n",
    "                for shape, tensors in grouped.items()\n",
    "            }\n",
    "    return processed\n",
    "\n",
    "def collect_activations(model_fn):\n",
    "    \"\"\"Collects activations during a model function execution.\"\"\"\n",
    "    print(\"\\nStarting activation collection\")\n",
    "    activations = defaultdict(lambda: defaultdict(list))\n",
    "    \n",
    "    patcher = install()\n",
    "    with patcher():\n",
    "        def run_in_greenlet():\n",
    "            try:\n",
    "                print(\"Starting model execution in greenlet...\")\n",
    "                return model_fn()\n",
    "            except Exception as e:\n",
    "                print(f\"Error in greenlet execution: {e}\")\n",
    "                traceback.print_exc()\n",
    "                raise\n",
    "\n",
    "        glet = safe_greenlet.SafeGreenlet(run_in_greenlet)\n",
    "        with glet:\n",
    "            result = glet.switch()\n",
    "            while glet:\n",
    "                try:\n",
    "                    site, value, name = result\n",
    "                    if torch.is_tensor(value):\n",
    "                        activations[site][name].append(value.detach().cpu())\n",
    "                    result = glet.switch(value)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error during activation collection: {e}\")\n",
    "                    traceback.print_exc()\n",
    "                    break\n",
    "    \n",
    "    print(f\"Collection complete. Found sites: {list(activations.keys())}\")\n",
    "    return _process_activations(activations), result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a25922d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_activations(activations, with_shape=False):\n",
    "    for site, tensors in activations.items():\n",
    "        print(f\"{site}:\")\n",
    "        for name, tensor_dict in tensors.items():\n",
    "            print(f\"\\tName: {name}\")\n",
    "            if with_shape:\n",
    "                for shape, tensor in tensor_dict.items():\n",
    "                    print(f\"\\t\\tShape {shape}: {tensor.shape[0]} tensor{'s' if tensor.shape[0] > 1 else ''}\")\n",
    "        print()\n",
    "\n",
    "def view_layer_paths(activations):\n",
    "    \"\"\"Displays activations organized by full layer paths.\"\"\"\n",
    "    prefix = \"Activations by Layer Path:\"\n",
    "    print(f\"\\n{prefix}\\n{'-' * len(prefix)}\")\n",
    "    \n",
    "    sections = {\n",
    "        'encoder': {'attention': set(), 'ffn': set()},\n",
    "        'decoder': {'attention': set(), 'ffn': set(), 'encoder_attn': set()}\n",
    "    }\n",
    "    \n",
    "    def categorize_path(path):\n",
    "            \"\"\"Categorizes paths into appropriate sections and components.\"\"\"\n",
    "            if 'encoder.' in path:\n",
    "                if any(x in path for x in ['q_lin', 'k_lin', 'v_lin', 'out_lin']):\n",
    "                    sections['encoder']['attention'].add(path)\n",
    "                elif any(x in path for x in ['lin1', 'lin2']):\n",
    "                    sections['encoder']['ffn'].add(path)\n",
    "            elif 'decoder.' in path:\n",
    "                if 'encoder_attn' in path:\n",
    "                    sections['decoder']['encoder_attn'].add(path)\n",
    "                elif any(x in path for x in ['q_lin', 'k_lin', 'v_lin', 'out_lin']):\n",
    "                    sections['decoder']['attention'].add(path)\n",
    "                elif any(x in path for x in ['lin1', 'lin2']):\n",
    "                    sections['decoder']['ffn'].add(path)\n",
    "    \n",
    "    for site_data in activations.values():\n",
    "        for path in filter(None, site_data.keys()):\n",
    "            categorize_path(path)\n",
    "    \n",
    "    for section, components in sections.items():\n",
    "        print(f\"{section.upper()}:\")\n",
    "        for component_type, paths in components.items():\n",
    "            if not paths:\n",
    "                continue\n",
    "\n",
    "            print(f\"\\t{component_type}:\")\n",
    "            for path in sorted(set(paths)):\n",
    "                print(f\"\\t\\t{path}:\")\n",
    "                for _, site_data in activations.items():\n",
    "                    if path in site_data:\n",
    "                        for shape, tensor in site_data[path].items():\n",
    "                            print(f\"\\t\\t\\tShape {shape}: {tensor.shape[0]} activations\")\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d142b96-39b5-4639-90e2-f9fa7a1bc65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing patches... Patches installed successfully\n"
     ]
    }
   ],
   "source": [
    "_ = install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "607b70d4-e439-48c9-9725-3674c70f1487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at odeformer.pt\n",
      "Loaded pretrained model\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Set\n",
    "import re\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "# Initialize the Symbolic Transformer Regressor model\n",
    "# `from_pretrained=True` loads a pre-trained version of the model\n",
    "model = SymbolicTransformerRegressor(from_pretrained=True)\n",
    "\n",
    "@dataclass\n",
    "class Keys:\n",
    "    \"\"\"\n",
    "    Stores configuration settings for filtering activations.\n",
    "\n",
    "    Attributes:\n",
    "        encoders (Set[int]): Set of encoder layer indices to keep.\n",
    "        decoders (Set[int]): Set of decoder layer indices to keep.\n",
    "        encoder_attn (Set[str]): Set of attention-related activations to collect. \n",
    "                                Can contain any of: ['attn_scores', 'attn_probs', 'attn_output'].\n",
    "        cross_attention (bool): Whether to keep cross-attention activations.\n",
    "\n",
    "        to_collect (Dict[str, bool]): Dictionary mapping activation types to boolean values\n",
    "                                      indicating whether to collect them.\n",
    "    \"\"\"\n",
    "    encoders: Set[int] = field(default_factory=set)\n",
    "    decoders: Set[int] = field(default_factory=set)\n",
    "    encoder_attn: Set[str] = field(default_factory=set)\n",
    "    cross_attention: bool = False\n",
    "\n",
    "    to_collect: Dict[str, bool] = field(default_factory=lambda: {\n",
    "        'attn_scores': False,\n",
    "        'attn_probs': False,\n",
    "        'attn_output': False,\n",
    "        'attn_mlp_output': False,\n",
    "        'mlp_input': False,\n",
    "        'mlp_output': False\n",
    "    })\n",
    "\n",
    "    def __post_init__(self):\n",
    "        \"\"\"\n",
    "        Ensures that `to_collect` always contains all possible keys with default values.\n",
    "        This prevents missing keys from causing errors when accessing `to_collect`.\n",
    "        \"\"\"\n",
    "        default_flags = {\n",
    "            'attn_scores': False,\n",
    "            'attn_probs': False,\n",
    "            'attn_output': False,\n",
    "            'attn_mlp_output': False,\n",
    "            'mlp_input': False,\n",
    "            'mlp_output': False\n",
    "        }\n",
    "\n",
    "        # Merge user-defined `to_collect` values with default values\n",
    "        self.to_collect = {**default_flags, **self.to_collect}\n",
    "\n",
    "    def _encoders_attn_to_remove(self) -> Set[str]:\n",
    "        \"\"\"\n",
    "        Determines which attention types should be removed from activations.\n",
    "\n",
    "        Returns:\n",
    "            Set[str]: A set of attention keys to remove.\n",
    "        \"\"\"\n",
    "        return {'attn_scores', 'attn_probs', 'attn_output'} - self.encoder_attn\n",
    "\n",
    "\n",
    "    def filter(self, activations: Dict[str, Dict[str, torch.Tensor]]) -> Dict[str, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Filters activations based on the `to_collect` settings and encoder/decoder indices.\n",
    "\n",
    "        Args:\n",
    "            activations (Dict[str, Dict[str, torch.Tensor]]): \n",
    "                Dictionary containing activation tensors, structured as:\n",
    "                {\n",
    "                    \"attn_scores\": { \"encoder.attentions.0.scores\": tensor, ... },\n",
    "                    \"attn_probs\": { \"encoder.attentions.1.weights\": tensor, ... },\n",
    "                    ...\n",
    "                }\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Dict[str, torch.Tensor]]: The filtered activations dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # Keep only activation keys that are enabled in `to_collect`\n",
    "        activations = {key: sub_dict for key, sub_dict in activations.items() if self.to_collect.get(key, False)}\n",
    "\n",
    "        # Remove encoder attention entries that are NOT specified in `encoder_attn`\n",
    "        if (to_remove := self._encoders_attn_to_remove()):\n",
    "            for key in filter(lambda k: k in activations, to_remove):\n",
    "                activations[key] = {\n",
    "                    k: v for k, v in activations[key].items() \n",
    "                    if 'encoder_attn' not in k\n",
    "                }\n",
    "\n",
    "        # If `cross_attention` is disabled, remove all `cross_attention` activations from 'attn_mlp_output'\n",
    "        if not self.cross_attention and (mlp_output := activations.get('attn_mlp_output')):\n",
    "            activations['attn_mlp_output'] = {\n",
    "                k: v for k, v in mlp_output.items() \n",
    "                if 'cross_attention' not in k\n",
    "            }\n",
    "\n",
    "        def check(x: str) -> bool:\n",
    "            \"\"\"\n",
    "            Determines whether a given activation key corresponds to a valid encoder or decoder layer.\n",
    "\n",
    "            Args:\n",
    "                x (str): The activation key, e.g., 'encoder.attentions.3.scores'.\n",
    "\n",
    "            Returns:\n",
    "                bool: True if the activation should be kept, False otherwise.\n",
    "            \"\"\"\n",
    "            if not (match := re.search(r\"\\d+\", x)):  # Extracts the first number in the key\n",
    "                return False\n",
    "            \n",
    "            layer = int(match.group())  # Convert extracted number to integer\n",
    "            indices = self.encoders if x.startswith(\"encoder\") else self.decoders\n",
    "\n",
    "            return layer in indices  # Keep only layers that exist in `self.encoders` or `self.decoders`\n",
    "\n",
    "        # Apply layer filtering to each activation type\n",
    "        return {\n",
    "            key: {k: v for k, v in values.items() if check(k)}\n",
    "            for key, values in activations.items()\n",
    "        }\n",
    "\n",
    "\n",
    "def collect(\n",
    "    input_path: str, \n",
    "    output_path: str, \n",
    "    model_args: dict = None,\n",
    "    keys: Keys = Keys\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes symbolic regression data, extracts activations, filters them using `Keys`,\n",
    "    and saves the processed activations.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to the input `.pkl` file containing solutions.\n",
    "        output_path (str): Path where the processed activations will be saved.\n",
    "        model_args (dict, optional): Arguments for configuring the symbolic transformer model.\n",
    "        keys (Keys): An instance of the `Keys` class to filter activations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load symbolic regression solutions from the input file\n",
    "    with open(input_path, 'rb') as file:\n",
    "        solutions = pickle.load(file)\n",
    "\n",
    "    # Set model hyperparameters if not provided\n",
    "    model_args = model_args or {\n",
    "        'beam_size': 20, \n",
    "        'beam_temperature': 0.1\n",
    "    }\n",
    "    model.set_model_args(model_args)\n",
    "    \n",
    "    collected_act = []\n",
    "\n",
    "    for solution in solutions:\n",
    "        trajectory = solution['solution']\n",
    "        times = solution['time_points']\n",
    "\n",
    "        fit_activations, outputs = collect_activations_during_fit(model, times, trajectory)\n",
    "        fit_activations = keys.filter(fit_activations)\n",
    "\n",
    "        collected_act.append(fit_activations)\n",
    "\n",
    "    with open(output_path, 'wb') as file:\n",
    "        pickle.dump(collected_act, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5f46b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n",
      "\n",
      "Starting activation collection\n",
      "Installing patches... Patches installed successfully\n",
      "Starting model execution in greenlet...\n",
      "Collection complete. Found sites: [<Site.ATTN_SCORES: 'attn_scores'>, <Site.ATTN_PROBS: 'attn_probs'>, <Site.ATTN_OUTPUT: 'attn_output'>, <Site.ATTN_MLP_OUTPUT: 'attn_mlp_output'>, <Site.MLP_INPUT: 'mlp_input'>, <Site.MLP_OUTPUT: 'mlp_output'>]\n"
     ]
    }
   ],
   "source": [
    "collect('lotka_volterra.pkl', 'act.pkl', keys=Keys(\n",
    "    decoders=[0,1],\n",
    "    encoder_attn={'attn_scores'},\n",
    "    cross_attention=True,\n",
    "    to_collect={\n",
    "        'attn_scores': True,\n",
    "        'attn_probs': True,\n",
    "        'attn_mlp_output': True\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f839640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attn_scores\n",
      "\tdecoder.attentions.0.scores\n",
      "\tdecoder.encoder_attn.0.scores\n",
      "\tdecoder.attentions.1.scores\n",
      "\tdecoder.encoder_attn.1.scores\n",
      "attn_probs\n",
      "\tdecoder.attentions.0.weights\n",
      "\tdecoder.attentions.1.weights\n",
      "attn_mlp_output\n",
      "\tdecoder.outer.attention_layer0\n",
      "\tdecoder.outer.cross_attention0\n",
      "\tdecoder.outer.attention_layer1\n",
      "\tdecoder.outer.cross_attention1\n"
     ]
    }
   ],
   "source": [
    "with open('act.pkl', 'rb') as file:\n",
    "    collected_act = pickle.load(file)\n",
    "\n",
    "for k, v in collected_act[0].items():\n",
    "    print(k)\n",
    "    for k1 in v:\n",
    "        print(f'\\t{k1}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
