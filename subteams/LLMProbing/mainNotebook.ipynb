{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pretrained model at odeformer.pt\n",
      "Loaded pretrained model\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import odeformer\n",
    "    from odeformer.model import SymbolicTransformerRegressor\n",
    "except ImportError:\n",
    "    print(\"Error: Please install odeformer package first using 'pip install odeformer'\")\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    dstr = SymbolicTransformerRegressor(from_pretrained=True)\n",
    "    model_args = {'beam_size': 50, 'beam_temperature': 0.1}\n",
    "    dstr.set_model_args(model_args)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing the model: {str(e)}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: encoder_attention_0, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm1_0, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_ffn_0, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm2_0, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_attention_1, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm1_1, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_ffn_1, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm2_1, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_attention_2, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm1_2, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_ffn_2, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm2_2, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_attention_3, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_layer_norm1_3, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: encoder_ffn_3, Output Shape: torch.Size([1, 50, 256])\n",
      "tensor([[[-0.1826, -0.5362, -0.3814,  ..., -0.6155, -1.0798, -0.4103],\n",
      "         [-0.4252, -0.2870, -0.6403,  ..., -0.5998,  0.0232,  0.0650],\n",
      "         [-0.8360, -1.0089, -0.3325,  ..., -0.8566, -0.6977, -0.2058],\n",
      "         ...,\n",
      "         [-0.8557, -0.6245, -0.4693,  ..., -0.2130, -0.7553,  0.3406],\n",
      "         [-0.9057, -0.2864, -0.3438,  ..., -0.3431, -0.1551, -0.1213],\n",
      "         [-0.4365, -1.2586, -0.6744,  ..., -0.5009, -0.9241, -0.0826]]])\n",
      "\n",
      "\n",
      "Layer: encoder_layer_norm2_3, Output Shape: torch.Size([1, 50, 256])\n",
      "Layer: decoder_attention_0, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_0, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_0, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_0, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_1, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_1, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_1, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_1, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_2, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_2, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_2, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_2, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_3, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_3, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_3, Output Shape: torch.Size([50, 1, 512])\n",
      "tensor([[[-0.0343,  0.1481,  0.4306,  ...,  0.0578,  0.0145, -0.0996]],\n",
      "\n",
      "        [[-0.0365,  0.1478,  0.4310,  ...,  0.0581,  0.0153, -0.0996]],\n",
      "\n",
      "        [[-0.0365,  0.1483,  0.4312,  ...,  0.0577,  0.0152, -0.0995]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0355,  0.1483,  0.4309,  ...,  0.0577,  0.0150, -0.0996]],\n",
      "\n",
      "        [[-0.0361,  0.1485,  0.4313,  ...,  0.0576,  0.0152, -0.0996]],\n",
      "\n",
      "        [[-0.0364,  0.1481,  0.4310,  ...,  0.0579,  0.0153, -0.0997]]])\n",
      "\n",
      "\n",
      "Layer: decoder_layer_norm2_3, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_4, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_4, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_4, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_4, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_5, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_5, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_5, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_5, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_6, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_6, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_6, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_6, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_7, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_7, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_7, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_7, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_8, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_8, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_8, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_8, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_9, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_9, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_9, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_9, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_10, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_10, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_10, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_10, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_attention_11, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm1_11, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_ffn_11, Output Shape: torch.Size([50, 1, 512])\n",
      "Layer: decoder_layer_norm2_11, Output Shape: torch.Size([50, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch, numpy as np\n",
    "\n",
    "\n",
    "layer_outputs = {}\n",
    "\n",
    "# Function to store the output of each layer\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    layer_outputs[layer_name] = output.detach().cpu()\n",
    "\n",
    "# Registering hooks for layers in the encoder and decoder\n",
    "def register_hooks(model_part, part_name):\n",
    "    for idx, module in enumerate(model_part.attentions):  # MultiHeadAttention layers\n",
    "        layer_name = f\"{part_name}_attention_{idx}\"\n",
    "        module.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "    for idx, module in enumerate(model_part.ffns):  # FeedForward layers\n",
    "        layer_name = f\"{part_name}_ffn_{idx}\"\n",
    "        module.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "    for idx, module in enumerate(model_part.layer_norm1):  # LayerNorm 1 layers\n",
    "        layer_name = f\"{part_name}_layer_norm1_{idx}\"\n",
    "        module.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "    for idx, module in enumerate(model_part.layer_norm2):  # LayerNorm 2 layers\n",
    "        layer_name = f\"{part_name}_layer_norm2_{idx}\"\n",
    "        module.register_forward_hook(lambda module, input, output, name=layer_name: hook_fn(module, input, output, name))\n",
    "\n",
    "# Registering hooks for the encoder and decoder parts\n",
    "register_hooks(dstr.model.encoder, 'encoder')\n",
    "register_hooks(dstr.model.decoder, 'decoder')\n",
    "\n",
    "times = np.linspace(0, 10, 50)\n",
    "x = 2.3 * np.cos(times + 0.5)\n",
    "y = 1.2 * np.sin(times + 0.1)\n",
    "trajectory = np.stack([x, y], axis=1)\n",
    "\n",
    "# Passing data through the model to capture layer outputs\n",
    "with torch.no_grad():\n",
    "    dstr.fit(times, trajectory)\n",
    "\n",
    "# Now, layer_outputs contains the outputs of the layers\n",
    "for layer_name, output in layer_outputs.items():\n",
    "    print(f\"Layer: {layer_name}, Output Shape: {output.shape}\")\n",
    "    if (layer_name in (\"encoder_ffn_3\",\"decoder_ffn_3\")):\n",
    "      print(output)\n",
    "      print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLMProbing)",
   "language": "python",
   "name": "llmprobing_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
